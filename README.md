# QA 작업을 위한 LLM 파인튜닝 방법

## **1. 전체적인 구조 설명**

### 데이터셋의 구조

---

데이터셋은 기사와 기사에 대한 질문, 답으로 이루어져있었으며, 해당 데이터는 저작권 때문에 업로드하지 않겠습니다.  
따라서 해당 프로젝트는 context와 이에 대응되는 QA로 되어있는 데이터셋에 적합합니다.  
LLM 모델을 이용해 QA task에 이용하시면 됩니다.

### Training

---

1. 데이터셋 구조 파악 및 데이터셋으로 만들기
2. 허깅페이스 로그인 후 허깅페이스에서 모델 불러오기
3. QLoRA 파리미터 설정
4. 학습 인자(하이퍼 파라미터) 설정
5. Trainer 이용해 학습 시작

### Test(Inference)

---

1. 체크포인트(학습 후 저장된) 모델 불러오기
2. 출력된 결과의 노이즈 제거를 위한 함수 정의
3. 모델의 디코딩 방법(beam search, top_k, top_p, etc.) 정의
4. test 데이터 토큰화 -> 모델 추론 -> 역토큰화 -> 노이즈 제거 를 실행하는 파이프라인 함수 정의
5. 추론 실행

## **2. Unsloth와 QLoRA**

## **Unsloth**

unsloth는 VRAM을 적게 사용하면서 학습을 시킬 수 있는 방법 중의 하나입니다.  
이때 Unsloth로 학습을 하기 위해서는

```python
FastLanguageModel.from_pretrained()
```

메서드를 이용해서 모델을 불러와주어야합니다.  
만약 Unsloth 를 사용하지 않는다면,

```python
AutoModelForCausalLM.from_pretrained()
```

이용해서 불러와주면 됩니다.

## **QLoRA**

QLoRA는 양자화(Quantization)+LoRA의 약자로서 양자화를 통해 모델을 경량화하여, 학습 및 추론을 빠르게 하며,  
LoRA를 통해 모델의 기존 가중치를 튜닝하는 풀파인튜닝이 아닌, 모델의 어텐션 유닛마다 새로운 low rank adapter를 붙임으로서 적은 파라미터를 학습함에도 불구하고 전체를 풀파인튜닝하는 것과 비슷한 성능을 보이는 PEFT 의 방법 중 하나입니다.  
QLoRA를 사용하는 방법은 다음과 같습니다.

**1. 양자화로 모델 불러오기**

    ```python
    model = AutoModelForCausalLM.from_pretrained(
        config.base_model_name_or_path,
        torch_dtype="auto",
        load_in_4bit=True, #4비트 양자화를 이용합니다. 8비트 양자화도 가능합니다.
        low_cpu_mem_usage = True,
        token=True
    )
    ```

    양자화의 경우, 위와 같이 선언해도 되지만,
    **BitsAndBytesConfig** 객체를 생성 후 인자로 넘겨주기도합니다.

**2. LoRA 어댑터 붙이기**  
**LoRA config 정의**  
❗️ LoRA에는 r 값과 alpha 값이 중요합니다.  
이 두개의 값은 하이퍼 파라미터로 각자의 모델에 맞게 조정이 필요합니다.

<p align="center"><img src="./readme_image/image.png" width="200"></p>  
위의 구조와 같이 r의 값은 low rank의 r 값입니다. loras는 A, B 행렬 2개이 행렬곱으로 이루어지며, 이때 A: dxr, B: rxd 크기의 행렬로 이루어집니다. 이 두 행렬의 곱으로 원래의 가중치의 차원인 dxd 차원의 행렬이 만들어집니다. 즉, r의 크기에 따라 A,B 행렬의 크기를 줄일 수 있으며, 늘릴수록 계산해야할 파라미터가 많아지는 것을 의미합니다. 즉 이러한 과정은 원래의 가중치 행렬을 low-rank 행렬로 근사시킨다고 할 수 있습니다.
alpha 값은 스케일링 factor로서 보통 r의 2배 크기로 사용합니다.

```python
peft_params = LoraConfig(
    r=r,  # attention 계산에 사용되는 쿼리(query), 키(key), 밸류(value)의 차원을 결정. 이 값은 일반적으로 lora_alpha와 관련이 있으며, 주로 모델의 표현력과 계산 비용을 조정하는 데 사용
    lora_alpha=lora_alpha,  # attention 계산에 사용되는 가중치 행렬의 크기를 나타냄. 일반적으로 큰 값일수록 모델의 표현력이 높아지지만, 계산 비용이 증가
    lora_dropout=lora_dropout,  # attention 계산 시에 적용되는 드롭아웃의 확률을 지정. 드롭아웃은 모델의 과적합을 방지하고 일반화 성능을 향상시키는 데 사용
    target_modules=[ #모델의 어텐션 중 어느곳에 lora 어댑터를 붙일지를 설정
    "q_proj",
    "up_proj",
    "o_proj",
    "k_proj",
    "down_proj",
    "gate_proj",
    "v_proj"],
    bias="none",  # attention 계산의 편향(bias)을 지정. 여기서는 "none"으로 설정되어 있으므로, 어텐션 계산에 편향이 추가되지 않음.
    task_type="CAUSAL_LM",  # 모델이 수행할 작업 유형. 여기서는 모델이 인과 언어 모델링(causal language modeling) 작업을 수행. 이는 주어진 이전 토큰들을 기반으로 다음 토큰을 예측하는 작업을 의미
)
```

**LoRA 어댑터 모델에 적용하기**

```python
model = get_peft_model(model, lora_config)
```

# Feat. 2024 인하 인공지능 대회가 끝나고...

본 코드는 2024 인하 인공지능 대회를 위해 작성되었습니다. context를 읽고 질문에 답변하는 모델의 성능을 높이기 위해 다양한 기법을 사용하였습니다.

1. 파인튜닝을 하고싶어... => **QLoRA**
2. VRAM이 부족해... => **Unsloth**
3. 그래도 성능이 안나와... => **모델 변경, 디코딩방법 변화, 프롬프트 엔지니어링(zero-shot, few-shot, priciple of prompt), RAG**

## 대회 기간의 사고과정

단순히 모델을 사용하여 주어진 context에 답변을 하라고 하게되면, 너무 장황하게 답변하는 경향이 보였습니다. 이를 해결하기위해 파인튜닝 방법을 모색하였고, 부족한 VRAM에서도 학습이 가능한 QLoRA를 발견하게되었습니다.

하지만 이 방법은 최대 토큰 개수가 크게되면 VRAM이 부족해지는 단점이 있었습니다. 주어진 기사의 최대 토큰 길이는 1600이었지만, 주어진 VRAM에서는 1100까지밖에 학습이 되지 않았습니다. 하지만 무작정 주어진 context를 1100까지로 잘라 학습에 사용하게되면, 질문에서 찾고자 하는 정보가 있는 정보가 잘릴수도 있었기때문에 학습에 악영향을 줄 가능성이 있다고 판단하여 다른 방법을 구상했습니다. 이에 우선 일단 모든 context를 토큰화한뒤, 토큰의 개수별로 나누어 토큰길이별 데이터의 분포를 확인하였습니다. 이를 통해 토큰의 길이가 1100 이하가 되도록하는 데이터만을 학습데이터로 사용하여 이러한 문제는 해결할 수 있었습니다.

하지만 이 경우에도, 모델에 긴 context를 주어지면, 정답률이 떨어지는 모습을 보여 이를 해결하기 위해서는 근본적인 문제를 해결해야함을 깨달았습니다. 따라서 적은 VRAM으로도 학습을 가능하게해주는 Unsloth를 도입하였습니다. 단순히 도입하는 것만으로도 최대토큰의 개수를 늘릴 수 있었으며 이를 통해, 길이가 긴 context를 가지는 데이터셋도 학습에 사용할 수 있었습니다.

이후 더 높은 점수를 위해 llama3 8B, yanolja EEVE, solar 등 여러 모델을 가지고 파인튜닝을 진행하면서 점수를 확인하였습니다. 하지만 라마3 모델의 경우가 제일 점수가 잘 나오게되어 이를 이용해 대회를 진행하였습니다.

디코딩 방식으로 beam search, top_k, top_p 등 다양한 방법을 사용해가며 모델이 올바른 답변을 하도록 해당 값들을 조금씩 수정했습니다. 최종적으로는 beams search와 top_k를 이용해 최종 답변을 생성하였습니다.

## 발견한 점

처음에는 단순히 validation loss를 확인하면서 loss가 계속 감소할때까지 에포크를 많이 돌리면 좋다고 생각하였는데, 막상 에포크를 많이 돌리니 적게 돌렸을때의 가중치를 가지는 모델에 비해 성능이 좋지 못하였습니다. 단답 질문에는 답변을 잘 하였지만, 에포크를 많이 돌릴수록 이유를 묻는 형식의 질문에는 답변을 잘하지 못하는 모습을 보였습니다. 따라서 최종 답변제출은 0.45에포크를 학습한 가중치를 바탕으로 생성하게되었습니다.  
디코딩 방식에서 기본 방식은 그리디 방식이기 때문에 잘못된 정답이 나오는 경우, beam search를 3으로 하였을때, 가장 적절한 답이 나오는것을 확인하였습니다. 나머지 옵션의 경우에는 답변에서 그렇게 큰 차이를 보이지는 못하였습니다.

## 시도하였지만 실패한 것들

대회 규정 상 RAG를 사용해도 된다고 하길래 RAG를 시도해보았습니다. 사실 RAG는 모델이 학습하지 않은 최신 정보를 외부데이터베이스를 통해 가져오는 것이 원래의 목표인데 외부 데이터는 사용할 수 없었기때문에 주어진 약 30000개의 데이터를 데이터베이스로 하여 RAG를 사용해보았습니다.

-   방법 1. **retriever를 통해 주어진 질문과 유사한 문단을 30000개의 데이터에서 뽑아오기**  
    => 오히려 모델의 점수가 하락함.

-   방법 2. **주어진 질문에 해당하는 답이 있을 것 같은 문장을 주어진 context에서 몇개 추려 새롭게 context를 만들고 이후에 답을 찾기**  
    => 정답이 없는 문장만으로 구성되는 경우가 종종 존재해 답을 찾지 못하는 경우가 발생하여 점수가 하락함.

따라서 RAG는 큰 효용성을 보이지 못하여 철폐하였습니다.

## 대회가 끝나고 점수가 더 좋은 팀들의 발표에서 얻은 점

위의 방법을 총동원하여 얻은 점수는 87점이었으나, 대회 최고점수는 90점이어서 어떤 방법을 사용했는지 굉장히 궁금하였는데 시상식 때 방법을 소개하는 자리를 가져 궁금점을 해소할 수 있었습니다.

### 우리 팀과 달랐던 점

1. 앙상블 기법을 사용함.
2. 답변에 대해서만 loss를 부여함.

앙상블 기법은 3가지로 나뉘었다.

-   여러 모델의 답변을 기반으로 voting 하는 방법
-   한개의 모델을 프롬프팅만 달리해서 여러 답변을 뽑아내고 이를 voting 하는 방법
-   한개의 모델에서 beam search 를 통해 여러 답변을 얻어내고 이를 voting 하는 방법

해당 기법을 설명 듣고, 왜 앙상블 기법을 이렇게 생각해내지 못했을까 하는 아쉬움이 많이 들었다.  
또한 답변에 대해서만 loss를 부여하는 방법 또한 획기적이었으며, 듣는내내 아쉬움과 함께 감탄하게 되는 발표였던 것 같다.

앞으로는 어떤 task에 대해 모델을 구축한다고 할때, 앙상블 기법을 사용함과 동시에 loss를 해당 task에 맞도록 잘 조정해야겠다는 생각을 뼈저리게 하게된 대회였던 것 같다.
